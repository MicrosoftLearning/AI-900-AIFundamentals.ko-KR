{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 얼굴 감지 및 분석\n",
    "\n",
    "Computer Vision 솔루션은 종종 사람의 얼굴을 감지, 분석, 식별할 수 있는 AI(인공 지능) 솔루션을 필요로 합니다. 예를 들어 소매업체 Northwind Traders가 AI 서비스를 통해 매장을 모니터링하여 도움이 필요한 고객을 파악하고 직원을 보내 돕도록 하는 \"스마트 스토어\"를 구현하기로 결정했다고 가정해 보겠습니다. 이를 달성하기 위한 한 가지 방법은 얼굴 감지 및 분석을 수행하는 것입니다. 즉, 이미지에 얼굴이 있는지 확인하고, 얼굴이 있다면 그 특징을 분석하는 것입니다.\n",
    "\n",
    "![얼굴을 분석하는 로봇](./images/face_analysis.jpg)\n",
    "\n",
    "## Face Cognitive Service를 사용하여 얼굴 감지\n",
    "\n",
    "Northwind Traders가 만들고자 하는 스마트 스토어 시스템이 고객을 감지하고 고객의 얼굴 특징을 분석할 수 있어야 한다고 가정해 보세요. Microsoft Azure에서는 Azure Cognitive Services에 포함된 **Face**를 사용하여 이 작업을 수행할 수 있습니다.\n",
    "\n",
    "### Cognitive Services 리소스 만들기\n",
    "\n",
    "먼저 Azure 구독에서 **Cognitive Services** 리소스를 만들어 보겠습니다.\n",
    "\n",
    "> **참고**: 이미 Cognitive Services 리소스를 보유하고 있다면 Azure Portal에서 **빠른 시작** 페이지를 열고 키 및 엔드포인트를 아래의 셀로 복사하기만 하면 됩니다. 리소스가 없다면 아래의 단계를 따라 리소스를 만듭니다.\n",
    "\n",
    "1. 다른 브라우저 탭에서 Azure Portal (https://portal.azure.com) 을 열고 Microsoft 계정으로 로그인합니다.\n",
    "2. **&#65291;리소스 만들기** 단추를 클릭하고, *Cognitive Services*를 검색하고, 다음 설정을 사용하여 **Cognitive Services** 리소스를 만듭니다.\n",
    "    - **구독**: *사용자의 Azure 구독*.\n",
    "    - **리소스 그룹**: *고유한 이름의 새 리소스 그룹을 선택하거나 만듭니다*.\n",
    "    - **지역**: *사용 가능한 지역을 선택합니다*.\n",
    "    - **이름**: *고유한 이름을 입력합니다*.\n",
    "    - **가격 책정 계층**: S0\n",
    "    - **알림을 읽고 이해했음을 확인합니다**. 선택됨.\n",
    "3. 배포가 완료될 때까지 기다립니다. 그런 다음에 Cognitive Services 리소스로 이동하고, **개요** 페이지에서 링크를 클릭하여 서비스의 키를 관리합니다. 클라이언트 애플리케이션에서 Cognitive Services 리소스에 연결하려면 엔드포인트 및 키가 필요합니다.\n",
    "\n",
    "### Cognitive Services 리소스의 키 및 엔드포인트 가져오기\n",
    "\n",
    "Cognitive Services 리소스를 사용하려면 클라이언트 애플리케이션에 해당 엔드포인트 및 인증 키가 필요합니다.\n",
    "\n",
    "1. Azure Portal에 있는 Cognitive Service 리소스의 **키 및 엔드포인트** 페이지에서 리소스의 **Key1**을 복사하고 아래 코드에 붙여 넣어 **YOUR_COG_KEY**를 대체합니다.\n",
    "\n",
    "2. 리소스의 **엔드포인트**를 복사하고 아래 코드에 붙여 넣어 **YOUR_COG_ENDPOINT**를 대체합니다.\n",
    "\n",
    "3. 셀 실행 <span>&#9655;</span> 단추(셀 왼쪽 상단에 있음)를 클릭하여 아래의 셀에 있는 코드를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693964655
    }
   },
   "outputs": [],
   "source": [
    "cog_key = 'YOUR_COG_KEY'\n",
    "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
    "\n",
    "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Face 서비스를 사용하여 매장에서 사람 얼굴을 감지할 수 있는 Cognitive Services 리소스가 생겼습니다.\n",
    "\n",
    "아래의 코드 셀을 실행하여 예시를 확인해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693970079
    }
   },
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from python_code import faces\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Create a face detection client.\n",
    "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
    "\n",
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "\n",
    "# Display the faces (code in python_code/faces.py)\n",
    "faces.show_faces(image_path, detected_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "감지된 얼굴 각각에 고유한 ID가 할당되므로 애플리케이션은 감지된 각각의 얼굴을 식별할 수 있습니다.\n",
    "\n",
    "아래의 셀을 실행하여 좀 더 많은 쇼핑객 얼굴의 ID를 표시해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693970447
    }
   },
   "outputs": [],
   "source": [
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "\n",
    "# Display the faces (code in python_code/faces.py)\n",
    "faces.show_faces(image_path, detected_faces, show_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 얼굴 특성 분석\n",
    "\n",
    "Face는 얼굴을 감지하는 것 외에도 많은 일들을 할 수 있습니다. 얼굴 특징 및 표현을 분석하여 연령과 감정 상태를 파악할 수도 있습니다. 예를 들어 아래 코드를 실행하여 쇼핑객의 얼굴 특성을 분석해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693971321
    }
   },
   "outputs": [],
   "source": [
    "# Open an image\n",
    "image_path = os.path.join('data', 'face', 'store_cam1.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detect faces and specified facial attributes\n",
    "attributes = ['age', 'emotion']\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream, return_face_attributes=attributes)\n",
    "\n",
    "# Display the faces and attributes (code in python_code/faces.py)\n",
    "faces.show_face_attributes(image_path, detected_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지에서 고객에 대해 감지된 감정 점수를 바탕으로 판단할 때, 이 고객은 쇼핑 경험에 꽤 만족한 것처럼 보입니다.\n",
    "\n",
    "## 유사 얼굴 찾기 \n",
    "\n",
    "감지된 각 얼굴에 대해 생성된 얼굴 ID는 얼굴 감지를 개별적으로 식별하는 데 사용됩니다. 이 ID를 사용하여 감지된 얼굴을 이전에 감지된 얼굴과 비교하고 유사한 특징의 얼굴을 찾을 수 있습니다.\n",
    "\n",
    "예를 들어 아래의 셀을 실행하여 한 이미지에 있는 쇼핑객과 다른 이미지에 있는 쇼핑객을 비교하여 일치하는 얼굴을 찾아보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693972555
    }
   },
   "outputs": [],
   "source": [
    "# Get the ID of the first face in image 1\n",
    "image_1_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
    "image_1_stream = open(image_1_path, \"rb\")\n",
    "image_1_faces = face_client.face.detect_with_stream(image=image_1_stream)\n",
    "face_1 = image_1_faces[0]\n",
    "\n",
    "# Get the face IDs in a second image\n",
    "image_2_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
    "image_2_stream = open(image_2_path, \"rb\")\n",
    "image_2_faces = face_client.face.detect_with_stream(image=image_2_stream)\n",
    "image_2_face_ids = list(map(lambda face: face.face_id, image_2_faces))\n",
    "\n",
    "# Find faces in image 2 that are similar to the one in image 1\n",
    "similar_faces = face_client.face.find_similar(face_id=face_1.face_id, face_ids=image_2_face_ids)\n",
    "\n",
    "# Show the face in image 1, and similar faces in image 2(code in python_code/face.py)\n",
    "faces.show_similar_faces(image_1_path, face_1, image_2_path, image_2_faces, similar_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 얼굴 인식\n",
    "\n",
    "지금까지, Face가 얼굴과 얼굴 특징을 감지할 수 있고 서로 유사한 두 얼굴을 식별할 수 있다는 것을 확인했습니다. 이제 특정인의 얼굴을 인식하도록 Face를 학습시킬 수 있는 *얼굴 인식* 솔루션을 구현함으로써 한 단계 더 나아갈 수 있습니다. 이것은 소셜 미디어 애플리케이션에서 친구의 사진을 자동으로 태깅하거나 얼굴 인식을 생체 인식 확인 시스템의 일부분으로 사용하는 등 다양한 시나리오에서 유용할 수 있습니다.\n",
    "\n",
    "이것이 어떻게 작동하는지 살펴보기 위해 Northwind Traders 회사가 얼굴 인식을 사용하여 IT 부서의 인증된 직원에게만 보안 시스템 액세스를 허용하려고 한다고 가정해 보겠습니다.\n",
    "\n",
    "먼저, 인증된 직원을 나타내는 *사람 그룹*을 만들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693973492
    }
   },
   "outputs": [],
   "source": [
    "group_id = 'employee_group_id'\n",
    "try:\n",
    "    # Delete group if it already exists\n",
    "    face_client.person_group.delete(group_id)\n",
    "except Exception as ex:\n",
    "    print(ex.message)\n",
    "finally:\n",
    "    face_client.person_group.create(group_id, 'employees')\n",
    "    print ('Group created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*사람 그룹*이 있으면 그룹에 포함하려는 각 직원에 대해 *사람*을 추가한 후에 각각의 사람에 대한 여러 사진을 등록하여 Face가 각자의 얼굴 특징을 학습하도록 할 수 있습니다. 같은 사람을 여러 가지 포즈와 얼굴 표현으로 보여주는 이미지를 사용하는 것이 좋습니다.\n",
    "\n",
    "Wendell이라는 직원을 추가하고 이 직원의 사진을 세 장 등록하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693976898
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Add a person (Wendell) to the group\n",
    "wendell = face_client.person_group_person.create(group_id, 'Wendell')\n",
    "\n",
    "# Get photo's of Wendell\n",
    "folder = os.path.join('data', 'face', 'wendell')\n",
    "wendell_pics = os.listdir(folder)\n",
    "\n",
    "# Register the photos\n",
    "i = 0\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for pic in wendell_pics:\n",
    "    # Add each photo to person in person group\n",
    "    img_path = os.path.join(folder, pic)\n",
    "    img_stream = open(img_path, \"rb\")\n",
    "    face_client.person_group_person.add_face_from_stream(group_id, wendell.person_id, img_stream)\n",
    "\n",
    "    # Display each image\n",
    "    img = Image.open(img_path)\n",
    "    i +=1\n",
    "    a=fig.add_subplot(1,len(wendell_pics), i)\n",
    "    a.axis('off')\n",
    "    imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사람을 추가하고 사진을 등록했으면 이제 Face가 각 사람을 인식하도록 학습시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693977046
    }
   },
   "outputs": [],
   "source": [
    "face_client.person_group.train(group_id)\n",
    "print('Trained!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델 학습이 완료되었으므로 이 모델을 사용하여 이미지에서 인식된 얼굴을 식별할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1599693994820
    }
   },
   "outputs": [],
   "source": [
    "# Get the face IDs in a second image\n",
    "image_path = os.path.join('data', 'face', 'employees.jpg')\n",
    "image_stream = open(image_path, \"rb\")\n",
    "image_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "image_face_ids = list(map(lambda face: face.face_id, image_faces))\n",
    "\n",
    "# Get recognized face names\n",
    "face_names = {}\n",
    "recognized_faces = face_client.face.identify(image_face_ids, group_id)\n",
    "for face in recognized_faces:\n",
    "    person_name = face_client.person_group_person.get(group_id, face.candidates[0].person_id).name\n",
    "    face_names[face.face_id] = person_name\n",
    "\n",
    "# show recognized faces\n",
    "faces.show_recognized_faces(image_path, image_faces, face_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자세한 내용\n",
    "\n",
    "Face Cognitive Service에 대해 자세히 알아보려면 [Face 설명서](https://docs.microsoft.com/azure/cognitive-services/face/)를 참조하세요.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}